# 問題生成品質の評価基準

問題生成プロンプトが生成したテストセットの品質を評価するためのルーブリック。
Phase C の評価実験で使用する。

---

## 1. 評価次元

### 1.1 入力型整合性 (Input Type Alignment) — Weight: 必須

テスト入力がエージェントの期待入力型と一致するか。これが不一致の場合、他の全次元が無意味になる。

| Rating | 条件 |
|--------|------|
| **Pass** | 全テストシナリオの入力がエージェントの期待入力型と一致 |
| **Fail** | 1つでもエージェントの期待入力型と異なる入力がある |

**判定方法**: agent-type-taxonomy.md の種別判定フローでエージェントの種別を特定し、各テストシナリオの入力を突合する。

**Fail の場合**: テストセット全体を不合格とし、他の次元は評価しない。

### 1.2 タスク整合性 (Task Alignment) — Weight: 3.0

ルーブリック基準がエージェントの実際のタスクを測定しているか。

| Rating | 条件 |
|--------|------|
| **3 (Full)** | 全ルーブリック基準がエージェントの核心能力に直接対応。基準名・条件がエージェントの目的と整合 |
| **2 (Mostly)** | 80%以上の基準がエージェントの核心能力に対応。一部の基準が周辺的だが有害ではない |
| **1 (Partial)** | 50-79%の基準が対応。エージェントの目的と無関係な基準が複数ある |
| **0 (Misaligned)** | 50%未満の基準が対応。エージェントの目的と異なるタスクを測定している |

**判定方法**:
1. エージェント定義から核心能力カテゴリを抽出
2. 各ルーブリック基準をカテゴリに対応付け
3. 対応しない基準の割合を算出

### 1.3 識別力 (Discriminative Power) — Weight: 3.0

異なる品質のプロンプト間でスコア差が出るか。天井効果（全プロンプト満点）や床効果（全プロンプト0点）がないか。

| Rating | 条件 |
|--------|------|
| **3 (High)** | 意図的に弱化したプロンプトで実行した場合、baseline との平均スコア差が 2.0pt 以上 |
| **2 (Medium)** | スコア差が 1.0-2.0pt |
| **1 (Low)** | スコア差が 0.5-1.0pt（ノイズと区別困難） |
| **0 (None)** | スコア差が 0.5pt 未満、または天井/床効果が発生 |

**判定方法**:
1. 対象エージェントの baseline プロンプトで全シナリオを実行
2. baseline から意図的に劣化させたプロンプト（主要指示を削除したもの）で同じシナリオを実行
3. スコア差を測定

**注**: この次元のみ実際の実行が必要。Phase C では全バリアントに対してこの検証を行う。コストが高いため、初期スクリーニングでは 1.1 と 1.2 を先に評価し、Pass かつ Rating >= 2 のバリアントのみ識別力検証に進む。

### 1.4 難易度分布 (Difficulty Distribution) — Weight: 2.0

テストシナリオの難易度が適切に分布しているか。

| Rating | 条件 |
|--------|------|
| **3 (Optimal)** | 易2 / 中3 / 難1-3 の分布を達成。baseline で易は満点、難は 50-80% のスコア |
| **2 (Acceptable)** | 分布は概ね達成。baseline で難も 90%以上スコア（やや容易）、または易で 70% 未満（やや困難） |
| **1 (Skewed)** | 特定の難易度に偏り（例: 全シナリオが中難度）。または易と難のスコア差が 1.0pt 未満 |
| **0 (Flat)** | 全シナリオが同等の難易度。プロンプトの弱点を識別できない |

**判定方法**: baseline でのシナリオ別スコアを確認し、難易度ラベルとの整合性を検証。

### 1.5 カバレッジ (Capability Coverage) — Weight: 2.0

エージェントの主要能力を網羅しているか。

| Rating | 条件 |
|--------|------|
| **3 (Full)** | エージェントの全能力カテゴリ（3-5個）から各1シナリオ以上。カテゴリ間のバランスが均等 |
| **2 (Mostly)** | 全カテゴリをカバーするが、特定カテゴリに偏り（50%以上のシナリオが1カテゴリ） |
| **1 (Partial)** | 1-2カテゴリが未カバー |
| **0 (Narrow)** | 半数以上のカテゴリが未カバー |

**判定方法**: エージェント定義から能力カテゴリを抽出し、シナリオのカテゴリ分布を確認。

### 1.6 入力の現実性 (Input Realism) — Weight: 1.0

テスト入力が実際のユースケースに近いか。

| Rating | 条件 |
|--------|------|
| **3 (Natural)** | 全入力が実際のプロジェクトで出てきそうな自然な内容。人工的マーカーなし |
| **2 (Mostly)** | 大部分が自然だが、1-2シナリオにやや人工的な構成あり |
| **1 (Artificial)** | 複数のシナリオに不自然な要素（「TODO: fix」、意図的すぎるエラー等） |
| **0 (Contrived)** | 明らかに人工的な入力が多数。実際のユースケースとかけ離れている |

**判定方法**: テスト入力を読み、以下をチェック:
- 「TODO」「FIXME」等の明示的マーカーがないか
- ドメインが自然か（架空だが現実的なプロジェクトか）
- 入力の構成が実際のユーザーが提供するものに近いか

---

## 2. 総合スコアの算出

### ゲート条件

入力型整合性 (1.1) が **Fail** の場合、テストセット全体を **不合格** とする。総合スコアは算出しない。

### スコア算出（ゲート通過時）

```
total_score = Σ(rating × weight) / Σ(max_rating × weight) × 10

max = (3×3.0 + 3×3.0 + 3×2.0 + 3×2.0 + 3×1.0) = 33.0
```

| 総合スコア | 判定 |
|-----------|------|
| 8.0-10.0 | 高品質 — そのまま使用可能 |
| 6.0-7.9 | 中品質 — 部分的な改善で使用可能 |
| 4.0-5.9 | 低品質 — 大幅な改善が必要 |
| 0.0-3.9 | 不適格 — 再生成が必要 |

---

## 3. 評価手順

### Phase 1: 静的評価（実行不要）

1. **入力型整合性** (1.1): テスト入力の型とエージェントの期待入力型を突合
2. **タスク整合性** (1.2): ルーブリック基準とエージェント目的の対応を検証
3. **カバレッジ** (1.5): 能力カテゴリとシナリオの対応表を作成
4. **入力の現実性** (1.6): テスト入力の自然さを確認

→ 1.1 が Fail、または 1.2 が 0-1 の場合はここで不合格とする。

### Phase 2: 動的評価（実行必要）

5. **識別力** (1.3): baseline + 弱化プロンプトでスコア差を測定
6. **難易度分布** (1.4): baseline のシナリオ別スコアから分布を検証

---

## 4. 評価結果のフォーマット

```markdown
## 問題生成品質評価: {variant_name} × {agent_name}

### ゲート条件
- 入力型整合性: Pass / Fail
- エージェント種別: Type-{X}
- 期待入力型: {type}
- テスト入力型: {type}

### 次元別スコア

| 次元 | Rating | Weight | Score | 根拠 |
|------|--------|--------|-------|------|
| タスク整合性 | {0-3} | 3.0 | {rating×weight} | {1行の根拠} |
| 識別力 | {0-3} | 3.0 | {rating×weight} | {スコア差の数値} |
| 難易度分布 | {0-3} | 2.0 | {rating×weight} | {分布の概要} |
| カバレッジ | {0-3} | 2.0 | {rating×weight} | {N}/{M} カテゴリカバー |
| 入力の現実性 | {0-3} | 1.0 | {rating×weight} | {1行の根拠} |

### 総合スコア: {X.X} / 10.0 ({判定})

### 特記事項
- {改善が必要な点や注目すべきパターン}
```
