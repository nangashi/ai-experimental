# agent_create スキルの問題生成能力 — 妥当性評価と改善計画

## 1. 現状分析

### 1.1 問題の概要

agent_create の Phase 2（テストセット作成）において、エージェントの目的に合致しない問題を生成するケースが発生し、以降のラウンドの評価結果が無効化される事態が確認されている。

### 1.2 具体的な障害事例

**critic-effectiveness (Round 3-4)**:
- エージェント目的: 観点定義ファイルの有効性を評価するメタ評価エージェント
- 期待されるテスト入力: 観点定義（perspective定義）
- 実際に生成されたテスト入力: SmartHealth IoT Platform のシステム設計書
- 結果: 全プロンプトがメタ評価に逸脱し、観点定義の評価ではなく設計書の評価を実施。S2a は完全に機能せず (0.0pt)、N1a は Run 間でタスク理解が分岐 (SD=4.0)。2ラウンド分の評価が完全に無効化された。

knowledge.md 項目13に記録: 「テスト文書種別がエージェント目的と一致しない場合、バリアントの真のパフォーマンスは測定不可能」

### 1.3 現在の問題生成メカニズムの構造的欠陥

**現在のフロー** (phase2-test-set.md + test-scenario-guide.md):
1. ベースラインエージェント定義を読み込み、主要タスク・入力・出力・能力カテゴリを特定
2. test-scenario-guide.md のガイドラインに従い 5-8 個のシナリオを生成
3. ユーザー承認

**欠陥**:

| 欠陥 | 説明 | 影響度 |
|------|------|--------|
| **入力型の未検証** | エージェントが受け取る入力の種類（設計書/コード/観点定義/要件等）とテストシナリオの入力型が一致するかの検証がない | Critical |
| **エージェント種別の未分類** | レビューア・実装者・メタ評価者など、エージェントの種別に応じた問題生成戦略の分岐がない | Critical |
| **問題の識別力検証なし** | 生成された問題がプロンプト間の性能差を識別できるかの事前検証がない（天井効果・床効果のリスク） | High |
| **ルーブリック基準のタスク整合性検証なし** | 評価基準がエージェントの実際の能力を測定するものかの検証がない | High |
| **テスト入力の現実性チェック不足** | ガイドラインに「自然な内容」とあるが、エージェント種別ごとの「現実的な入力」の定義がない | Medium |

### 1.4 reviewer_optimize との比較

reviewer_optimize は同様の問題を以下で回避している:

| 要素 | reviewer_optimize | agent_create |
|------|-------------------|--------------|
| テスト入力型 | 明確に規定（設計書 150-250行 / コード 100-200行） | 未規定（20-100行の汎用入力） |
| 問題埋め込み | perspective の problem bank から具体例を参照 | 汎用ガイドラインのみ |
| 答え合わせ | answer-key (○/△/×判定基準) | rubric (Full/Partial/Miss) |
| 種別分岐 | design / code の2種別で生成方法が異なる | 分岐なし |
| ドメイン多様性 | knowledge.md 履歴から重複回避を強制 | テストセットは初回のみ（再利用） |
| 問題数 | 8-10問 + ボーナス機会 | 5-8シナリオ（固定） |

---

## 2. 改善計画

### Phase A: 現状の問題生成指示のサブエージェント化と分析基盤構築

**目的**: 問題生成ロジックを独立したサブエージェント定義として切り出し、バリアント生成・評価が可能な状態にする。

#### A-1. 問題生成プロンプトの抽出・整理

**作業内容**:
- 現在の `phase2-test-set.md`（32行）と `test-scenario-guide.md`（109行）の内容を統合し、問題生成サブエージェントの単体プロンプト `problem-generator.md` として切り出す
- この時点では内容の改善はせず、現行ロジックの忠実な移植のみ行う

**成果物**: `.task/problem-generation-improvement/prompts/v001-baseline.md`

#### A-2. エージェント種別タクソノミの策定

**作業内容**:
- 既存のエージェント実例とtest-set/knowledgeデータから、評価対象となりうるエージェントの種別を分類する
- 各種別について「入力型」「出力型」「核心能力」「適切なテスト構造」を定義する

**推定エージェント種別**:

| 種別 | 入力型 | 出力型 | 例 |
|------|--------|--------|-----|
| 設計書レビューア | システム設計書 | 問題指摘リスト | security-design-reviewer, performance-design-reviewer |
| コードレビューア | ソースコード | 問題指摘リスト | (code perspectives) |
| メタ評価エージェント | エージェント/観点定義 | 批評・評価レポート | critic-effectiveness, critic-generality, critic-clarity |
| 実装エージェント | 要件/仕様 | コード/設定ファイル | (issue_implement 等) |
| 分析エージェント | 要件/コードベース情報 | 分析レポート/計画 | plan-architect |
| 変換エージェント | 入力フォーマットA | 出力フォーマットB | (特定変換タスク) |

**成果物**: `.task/problem-generation-improvement/agent-type-taxonomy.md`

#### A-3. 問題品質の評価基準策定

**作業内容**:
- 問題生成結果の品質を評価するためのルーブリックを設計する
- 問題の「妥当性」を多角的に測定可能にする

**評価次元（案）**:

| 次元 | 定義 | 測定方法 |
|------|------|----------|
| **入力型整合性** | テスト入力がエージェントの期待入力型と一致するか | エージェント定義のinput仕様とテスト入力の型を突合 |
| **タスク整合性** | ルーブリック基準がエージェントのタスクを測定するか | 各基準がエージェント目的の能力カテゴリに対応するか検証 |
| **識別力** | 異なる品質のプロンプト間で得点差が出るか | 意図的に弱いプロンプトで実行し、baselineとの差を確認 |
| **難易度分布** | 易・中・難が適切に分布しているか | 全プロンプトで満点のシナリオ比率が 0-30% 以内か |
| **カバレッジ** | エージェントの主要能力を網羅しているか | 能力カテゴリとシナリオの対応表で未カバー確認 |
| **入力の現実性** | テスト入力が実際のユースケースに近いか | 人工的なマーカーや不自然な構成がないか |

**成果物**: `.task/problem-generation-improvement/evaluation-criteria.md`

---

### Phase B: バリアントパターンの検討と生成

**目的**: 問題生成プロンプトの改善バリアントを設計する。

#### B-1. 問題生成のバリアントパターン設計

**作業内容**:
- 現状分析で特定した欠陥を解消するためのバリアントアプローチを設計する
- reviewer_optimize の成功パターンを参考に、agent_create に適用可能な手法を選定する

**検討バリアント案**:

| ID | バリアント名 | アプローチ | 解決する欠陥 |
|----|-------------|-----------|-------------|
| V1 | type-aware | エージェント種別を先に分類し、種別に応じたテスト入力生成テンプレートを使用 | 入力型の未検証 |
| V2 | validation-gate | 生成後に入力型整合性・タスク整合性のセルフチェックステップを追加 | 入力型/タスク整合性の未検証 |
| V3 | problem-bank | reviewer_optimize 方式。エージェント定義から問題バンク（埋め込むべき問題リスト）を先に生成し、それをテスト入力に埋め込む | 問題の識別力不足 |
| V4 | answer-key | ルーブリックに加え、reviewer_optimize の ○/△/× 式の答え合わせキーを生成 | 採点の曖昧性 |
| V5 | discrimination-check | 弱いプロンプト（意図的に劣化させたもの）で試行し、識別力を事前検証 | 天井効果/床効果 |

**成果物**: `.task/problem-generation-improvement/prompts/v001-variant-{name}.md` (各バリアント)

---

### Phase C: エージェント種別ごとの評価実験

**目的**: 各バリアントの問題生成プロンプトを複数のエージェント種別に対して実行し、問題生成品質を比較評価する。

#### C-1. 評価対象エージェントの選定

**選定基準**:
- 種別タクソノミの各カテゴリから最低1つ
- 既存の評価データ（knowledge.md）があるものを優先（問題品質の後付け検証が可能）
- 入力型が明確に異なるペアを含める（対照群として有用）

**候補**:

| エージェント | 種別 | 既存データ | 選定理由 |
|-------------|------|-----------|----------|
| critic-effectiveness | メタ評価 | 3ラウンド（2無効） | 既知の障害事例。改善効果を直接測定可能 |
| critic-generality | メタ評価 | 3ラウンド | critic-effectivenessとの対照。テストセット品質に問題なし |
| security-design-reviewer | 設計書レビュー | 多数ラウンド | reviewer_optimize で実績豊富。問題生成の理想形として参照可能 |
| (新規作成エージェント) | 実装 or 分析 | なし | 新規作成フローでの問題生成品質を検証 |

#### C-2. 評価実験の実行

**実験設計**:

各バリアント (baseline + V1〜V5) × 各エージェント種別 (3-4種) の組み合わせで問題を生成し、Phase A-3 の評価基準で採点する。

```
問題生成プロンプト × 対象エージェント → テストセット → 評価基準で採点
```

**実行手順**:
1. 各バリアントの問題生成プロンプトをサブエージェントとして実行
2. 生成されたテストセットを Phase A-3 の評価基準で人手 + 自動採点
3. 特に「入力型整合性」と「タスク整合性」は必ず人手で確認（自動化困難）
4. 可能であれば、生成されたテストセットで実際にエージェントを評価し、結果の妥当性を後付け検証

**成果物**:
- `.task/problem-generation-improvement/results/{variant}-{agent-type}-evaluation.md`
- `.task/problem-generation-improvement/results/comparison-matrix.md`

---

### Phase D: 最良バリアントの選定とAgent化

**目的**: 評価結果から最良のアプローチを選定し、agent_create に統合可能な形にする。

#### D-1. 結果分析と選定

**分析観点**:
- エージェント種別ごとのバリアント性能比較
- 全種別で安定して高品質な問題を生成できるバリアントの特定
- 種別ごとに最適なバリアントが異なる場合、種別分岐の要否を判定

**判定ルール**:
- 入力型整合性 100%（必須条件。これを満たさないバリアントは不採用）
- タスク整合性スコアが baseline を上回る
- 識別力スコアが一定以上（具体的な閾値は Phase C の結果から設定）

#### D-2. 問題生成エージェントの最終化

**作業内容**:
- 選定バリアントを問題生成サブエージェント定義 `problem-generator.md` として整備
- エージェント種別タクソノミを組み込み、種別に応じた生成戦略の分岐を実装
- バリデーションゲート（入力型整合性チェック）を組み込む

**成果物**: `.task/problem-generation-improvement/problem-generator-final.md`

---

### Phase E: agent_create への統合

**目的**: 改善された問題生成エージェントを agent_create スキルに統合する。

#### E-1. agent_create の Phase 2 更新

**変更対象ファイル**:
- `.claude/skills/agent_create/SKILL.md` — Phase 2 のサブエージェント起動部分を更新
- `.claude/skills/agent_create/templates/phase2-test-set.md` — 新しい問題生成プロンプトに置換
- `.claude/skills/agent_create/test-scenario-guide.md` — エージェント種別タクソノミを統合して更新

**変更方針**:
- Phase 0 でエージェント種別を分類するステップを追加（入力型・出力型の明示的特定）
- Phase 2 のサブエージェント指示を更新し、種別に応じた問題生成を行う
- 生成後のバリデーションゲートを追加（入力型整合性・タスク整合性の自動チェック）
- バリデーション失敗時は再生成を1回試行し、それでも失敗ならユーザーに報告

#### E-2. 既存テストセットへの遡及対応

**対象**: critic-effectiveness（テスト設計問題で Round 3-4 が無効化されている）

- 改善された問題生成で新テストセットを生成
- 既存の有効な knowledge.md データは保持し、無効マークされた Round 3-4 のデータを根拠としないように knowledge.md を整理

---

## 3. 実行順序と依存関係

```
Phase A-1 (プロンプト抽出)
    ↓
Phase A-2 (エージェント種別タクソノミ) ←→ Phase A-3 (評価基準策定)  [並列可能]
    ↓
Phase B-1 (バリアント設計)
    ↓
Phase C-1 (評価対象選定)
    ↓
Phase C-2 (評価実験)
    ↓
Phase D-1 (結果分析)
    ↓
Phase D-2 (エージェント最終化)
    ↓
Phase E-1 (agent_create統合) → Phase E-2 (遡及対応)
```

## 4. リスクと軽減策

| リスク | 影響 | 軽減策 |
|--------|------|--------|
| バリアント間の性能差が小さく、明確な勝者が出ない | 改善効果が不明確 | Phase A-3 の評価基準を事前に具体化し、微小差でも判定可能にする |
| 問題生成のメタ評価自体が正確でない | 評価結果の信頼性低下 | 人手確認を必須とし、特に入力型整合性は自動化しない |
| エージェント種別タクソノミが網羅的でない | 未知の種別で問題再発 | タクソノミに「不明/その他」カテゴリを設け、該当時はユーザー確認を強制 |
| 改善されたプロンプトが長大化し、サブエージェントのコンテキスト消費増 | Phase 2 の実行コスト増 | コンテキスト節約原則に従い、テンプレート参照方式を維持 |

## 5. ファイル構成（計画）

```
.task/problem-generation-improvement/
├── plan.md                          # この計画書
├── agent-type-taxonomy.md           # Phase A-2: エージェント種別タクソノミ
├── evaluation-criteria.md           # Phase A-3: 問題品質の評価基準
├── prompts/
│   ├── v001-baseline.md             # Phase A-1: 現行ロジック移植
│   ├── v001-variant-type-aware.md   # Phase B-1: V1
│   ├── v001-variant-validation.md   # Phase B-1: V2
│   ├── v001-variant-problem-bank.md # Phase B-1: V3
│   └── ...
├── results/
│   ├── {variant}-{agent}-eval.md    # Phase C-2: 各実験結果
│   └── comparison-matrix.md         # Phase C-2: 比較マトリクス
├── problem-generator-final.md       # Phase D-2: 最終版プロンプト
└── integration-diff.md              # Phase E-1: agent_create への変更差分
```
