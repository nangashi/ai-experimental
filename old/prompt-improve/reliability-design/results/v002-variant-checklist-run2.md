# Reliability Design Review: IoT Device Management Platform

## Executive Summary

This review identifies **7 critical issues**, **5 significant issues**, and **4 moderate issues** across fault recovery design, data consistency, availability architecture, monitoring strategy, and deployment safety. The design demonstrates awareness of basic reliability patterns (Resilience4j, Rolling Update) but lacks explicit specification of critical mechanisms such as idempotency guarantees, circuit breaker configurations, distributed transaction handling, SPOF mitigation, SLO-based alerting, and rollback procedures.

---

## Critical Issues

### C1. Missing Idempotency Design for Sensor Data Ingestion

**Location:** Section 3 (Device Ingestion Service), Section 4 (sensor_measurements table)

**Issue:**
The design does not specify idempotency guarantees for sensor data ingestion. With 100,000 msg/sec throughput and retry mechanisms (Resilience4j mentioned in Section 2), duplicate messages during retries could result in:
- Duplicate time-series records in TimescaleDB
- Incorrect aggregation results in Kafka Streams processing
- False alerts from anomaly detection based on inflated metrics

The `sensor_measurements` table lacks deduplication mechanisms (e.g., unique constraint on `(device_id, time, metric_type)` or idempotency token columns).

**Impact:**
- **Data Integrity Failure:** Duplicate measurements corrupt analytics and business decisions
- **Cascading Alert Storms:** Inflated metric values trigger false anomaly alerts
- **Difficult Debugging:** No way to distinguish legitimate duplicate readings from retry-induced duplicates

**Countermeasures:**
1. **Add idempotency token** to MQTT message payload (e.g., `message_id` UUID generated by device)
2. **Implement deduplication layer** in Device Ingestion Service:
   - Use Redis with TTL (e.g., 5 minutes) to track processed message IDs
   - Check Redis before Kafka publish; skip if duplicate detected
3. **Add unique constraint** to TimescaleDB: `CREATE UNIQUE INDEX idx_unique_measurement ON sensor_measurements (device_id, time, metric_type);`
4. **Document idempotency contract** in API design (Section 5) specifying retry behavior expectations

**Reference:** Industry Standard Checklist → Distributed Systems Patterns → "Idempotent operation design for safe retries"

---

### C2. No Circuit Breaker Configuration Specified

**Location:** Section 2 (Resilience4j mentioned), Section 3 (Backend API dependencies)

**Issue:**
While Resilience4j is listed as a dependency, the design provides no circuit breaker configuration for critical external dependencies:
- PostgreSQL database calls (Device Management API)
- TimescaleDB writes (Stream Processing Service)
- Redis cache access (Backend API)
- S3 calls for firmware package storage (Firmware Update Service)

Without explicit thresholds, timeout values, and fallback behaviors, cascading failures are likely. For example, if PostgreSQL experiences slow queries, unbounded retries could exhaust connection pools and crash the Backend API.

**Impact:**
- **Cascading Service Failures:** Slow dependency propagates latency to all callers
- **Resource Exhaustion:** Thread pool/connection pool depletion during partial outages
- **Uncontrolled Retry Storms:** Retries without backoff overwhelm already-degraded dependencies

**Countermeasures:**
1. **Define circuit breaker configuration** for each dependency:
   - PostgreSQL: 50% error rate over 10 requests → OPEN state, 30s wait
   - TimescaleDB: 60% error rate over 20 requests → OPEN state, 60s wait (higher tolerance for time-series writes)
   - Redis: 70% error rate → OPEN state, 10s wait (cache misses are non-critical)
   - S3: 50% error rate → OPEN state, 60s wait
2. **Specify fallback behaviors:**
   - Redis OPEN → bypass cache, direct DB query with warning log
   - PostgreSQL OPEN → return 503 Service Unavailable with retry-after header
   - S3 OPEN → queue firmware updates, retry from persistent queue
3. **Document monitoring** of circuit breaker state changes (emit metrics to Micrometer)
4. **Add runbook entries** (Section 8) for circuit breaker OPEN state responses

**Reference:** Industry Standard Checklist → Distributed Systems Patterns → "Circuit breakers to prevent cascading failures"

---

### C3. Undefined Transaction Boundaries for Firmware Updates

**Location:** Section 4 (firmware_updates and device_update_status tables), Section 3 (Firmware Update Service)

**Issue:**
The firmware update workflow involves multi-step state transitions across two tables but lacks transaction boundary specification:
1. Insert into `firmware_updates` table
2. Insert multiple rows into `device_update_status` for target devices
3. Trigger OTA update to devices
4. Update `device_update_status.status` based on device responses

If Backend API crashes between steps 1-2, orphaned firmware_updates records with no associated devices result in "zombie" update jobs. If crashes occur between steps 2-3, devices show "in_progress" status indefinitely with no actual update triggered.

**Impact:**
- **Inconsistent State:** Database shows update initiated but devices never receive packages
- **Operational Confusion:** Operators cannot distinguish failed updates from in-progress updates
- **Manual Cleanup Required:** No automated reconciliation to detect and fix inconsistent states

**Countermeasures:**
1. **Define transaction scope** for update initiation:
   ```sql
   BEGIN TRANSACTION ISOLATION LEVEL READ COMMITTED;
   INSERT INTO firmware_updates (...) RETURNING update_id;
   INSERT INTO device_update_status (device_id, update_id, status, started_at)
   SELECT device_id, :update_id, 'scheduled', NOW()
   FROM devices WHERE device_id IN (:device_ids);
   COMMIT;
   ```
2. **Implement idempotent update trigger**:
   - Add `UPDATE firmware_updates SET triggered_at = NOW() WHERE update_id = :id AND triggered_at IS NULL;`
   - Only send OTA commands if `triggered_at` was NULL (first trigger attempt)
3. **Add background reconciliation job**:
   - Periodically scan for `device_update_status` records in "scheduled" state older than 10 minutes
   - Re-trigger OTA commands or mark as "failed" with timeout reason
4. **Add distributed tracing** (Jaeger/Zipkin) to correlate database transactions with OTA messaging

**Reference:** Industry Standard Checklist → Data Reliability → "Transaction boundaries and ACID guarantees where needed"

---

### C4. No Kafka Consumer Offset Commit Strategy Defined

**Location:** Section 3 (Stream Processing Service with Kafka Streams)

**Issue:**
The design specifies Kafka Streams for processing but does not define offset commit strategies or failure handling. Without explicit offset management:
- **At-least-once delivery** is implicit, leading to duplicate processing if Stream Processor crashes mid-batch
- No specification of commit frequency (per-message vs. batch commits)
- No handling of poison pill messages (malformed sensor data that causes repeated crashes)

If a Stream Processor instance crashes after writing to TimescaleDB but before committing Kafka offsets, the same messages are reprocessed, creating duplicate time-series records (related to C1).

**Impact:**
- **Data Duplication:** Reprocessed messages insert duplicate TimescaleDB rows
- **Processing Stalls:** Poison pill messages cause crash loops, blocking partition progress
- **Inconsistent State:** Some messages processed multiple times, others skipped due to state store corruption

**Countermeasures:**
1. **Define offset commit policy:**
   - Use Kafka Streams default (commit after state store flush)
   - Configure `commit.interval.ms` to 1000ms (balance latency vs. reprocessing risk)
   - Enable exactly-once semantics (EOS) via `processing.guarantee=exactly_once_v2`
2. **Implement poison pill handling:**
   - Wrap stream processing logic in try-catch
   - Send malformed messages to dead-letter topic (`sensor-data-dlq`)
   - Log device_id and error reason for operational review
3. **Add offset lag monitoring:**
   - Track consumer group lag per partition
   - Alert if lag exceeds 10,000 messages (indicates processing bottleneck)
4. **Document recovery procedure** in Section 8:
   - For data corruption: reset consumer group offsets to specific timestamp, reprocess from checkpoint

**Reference:** Industry Standard Checklist → Distributed Systems Patterns → "Idempotent operation design for safe retries", SRE Best Practices → "Incident response runbooks"

---

### C5. Single Point of Failure: PostgreSQL Database

**Location:** Section 2 (PostgreSQL 15), Section 3 (Backend API → PostgreSQL)

**Issue:**
PostgreSQL is specified without redundancy design. If the primary database becomes unavailable:
- Device Management API cannot serve read or write requests (CRUD operations, device queries)
- Firmware update scheduling fails
- No automatic failover mechanism mentioned

While AWS RDS is listed, the design does not specify Multi-AZ deployment, read replicas, or failover automation. With a 99.9% availability target (Section 7), a single database instance is insufficient (AWS RDS SLA is 99.95% for Multi-AZ, but single-AZ is lower).

**Impact:**
- **Service Outage:** All device management operations unavailable during database downtime
- **SLA Violation:** Cannot achieve 99.9% availability with single database instance
- **Data Loss Risk:** No mention of backup/restore procedures or RPO/RTO targets

**Countermeasures:**
1. **Deploy PostgreSQL in Multi-AZ configuration** (AWS RDS):
   - Primary instance in ap-northeast-1a
   - Synchronous standby replica in ap-northeast-1c
   - Automatic failover with 60-120s recovery time
2. **Add read replicas** for query load distribution:
   - 2 read replicas for GET /api/v1/devices queries
   - Update application to route reads to replicas (Spring Data JPA ReadOnlyRoutingDataSource)
3. **Define backup strategy**:
   - RDS automated backups with 7-day retention
   - Daily manual snapshots retained for 30 days
   - **RPO: 5 minutes** (point-in-time recovery via RDS backups)
   - **RTO: 30 minutes** (restore from latest snapshot + apply transaction logs)
4. **Add database connection pooling config**:
   - HikariCP max pool size: 20 connections per API instance
   - Connection timeout: 30 seconds
   - Validation query: `SELECT 1`

**Reference:** Industry Standard Checklist → Infrastructure Resilience → "Single point of failure (SPOF) identification and mitigation", "Redundancy at appropriate levels"

---

### C6. No SLO-Based Alerting Thresholds

**Location:** Section 7 (パフォーマンス目標), Section 8 (アラート設定)

**Issue:**
Section 7 defines performance targets (P95 < 500ms, P99 < 1000ms) and availability target (99.9%), but Section 8's alert definitions use vague criteria:
- P1: "サービス全断" (no quantitative threshold)
- P2: "パフォーマンス低下" (no latency threshold tied to SLO)
- P3: "リソース枯渇の兆候" (no specific metric values)

Without SLO-based alert thresholds, teams cannot distinguish between noise and actionable incidents. For example, if P99 latency reaches 900ms (within SLO) but operators receive P2 alerts for "performance degradation," this creates alert fatigue.

**Impact:**
- **Alert Fatigue:** Operators ignore alerts due to false positives
- **Missed SLA Violations:** No alerts when actually breaching 99.9% availability target
- **Inconsistent Incident Response:** Teams lack objective criteria for escalation decisions

**Countermeasures:**
1. **Define SLO-based alert thresholds:**
   - **Error Budget Alert (P1):**
     - Trigger: Error rate > 0.1% sustained for 5 minutes (consumes 50% of monthly error budget)
     - Action: Page on-call engineer immediately
   - **Latency SLO Alert (P2):**
     - Trigger: P95 latency > 500ms for 10 minutes OR P99 latency > 1000ms for 5 minutes
     - Action: Notify on-call via Slack, begin investigation
   - **Availability Alert (P1):**
     - Trigger: Service returning 5xx errors at >1% rate for 3 minutes
     - Action: Page on-call, initiate incident response
2. **Add burn rate alerts** (SRE best practice):
   - Fast burn (1-hour window): consuming error budget at 14.4x rate → immediate page
   - Slow burn (6-hour window): consuming error budget at 6x rate → Slack notification
3. **Document alert response procedures** in Section 8:
   - Each alert type links to specific runbook entry
   - Runbooks include diagnostic commands, escalation criteria, rollback procedures
4. **Implement error budget dashboard:**
   - Real-time display of remaining error budget (percentage)
   - Historical burn rate trends

**Reference:** Industry Standard Checklist → SRE Best Practices → "Error budgets and SLO/SLA definitions with measurable thresholds"

---

### C7. No Rollback Procedure for Database Migrations

**Location:** Section 6 (デプロイメント), Section 4 (データモデル)

**Issue:**
Section 6 specifies Kubernetes Rolling Update but does not address database schema migration rollback. If a new application version introduces a breaking schema change:
- Adding a non-nullable column without default value causes old application pods to fail writes
- Removing a column still referenced by old code versions causes runtime errors during rolling update
- No mention of backward-compatible migration strategies (expand-contract pattern)

With Rolling Update's `maxUnavailable: 1` and `maxSurge: 1`, old and new application versions coexist temporarily, requiring schema changes to be compatible with both versions.

**Impact:**
- **Deployment Failures:** Schema incompatibility crashes application pods mid-rollout
- **Data Corruption Risk:** Partial schema changes with mixed application versions create inconsistent state
- **Extended Downtime:** Manual schema rollback requires service interruption

**Countermeasures:**
1. **Adopt expand-contract migration pattern:**
   - **Expand phase:** Add new columns/tables as nullable, deploy application version N
   - **Migrate phase:** Backfill data, deploy application version N+1 using new schema
   - **Contract phase:** Remove old columns after confirming version N+1 stability
2. **Define rollback procedure** in Section 6:
   - Maintain rollback SQL scripts in version control alongside migration scripts
   - Test rollback scripts in staging environment before production deployment
   - For breaking changes: require manual approval and scheduled maintenance window
3. **Add migration backward compatibility checks:**
   - Automated tests verify old application version can operate with new schema
   - CI pipeline blocks deployment if compatibility checks fail
4. **Document migration checklist** in deployment runbook:
   - [ ] Migration scripts reviewed for backward compatibility
   - [ ] Rollback scripts tested in staging
   - [ ] Database backup taken before migration
   - [ ] Monitoring dashboard ready to track deployment metrics

**Reference:** Industry Standard Checklist → Operational Safety → "Database migration backward compatibility", "Rollback procedures with rollback criteria"

---

## Significant Issues

### S1. Timeout Configuration Missing for External Dependencies

**Location:** Section 3 (全体構成), Section 5 (API設計)

**Issue:**
The design does not specify timeout values for external calls:
- Backend API → PostgreSQL queries
- Backend API → Redis cache operations
- Stream Processor → TimescaleDB writes
- Firmware Update Service → S3 firmware package downloads
- Frontend → Backend API calls

Without timeouts, slow dependencies cause thread pool exhaustion. For example, if PostgreSQL experiences slow queries (e.g., due to missing index), API threads block indefinitely, preventing new request handling.

**Impact:**
- **Resource Exhaustion:** All worker threads blocked on slow DB queries
- **Cascading Latency:** Upstream services (Frontend) experience extreme latency
- **Difficult Root Cause Analysis:** Timeouts provide clear failure signals vs. indefinite hangs

**Countermeasures:**
1. **Define timeout hierarchy** (each layer should have shorter timeout than caller):
   - PostgreSQL connection timeout: 5s, query timeout: 30s
   - Redis operations: 1s (cache is non-critical)
   - TimescaleDB writes: 10s (time-series inserts should be fast)
   - S3 GET requests: 60s (firmware packages are large)
   - Backend API → Database: 30s (includes connection acquisition + query)
   - Frontend → Backend API: 10s (user-facing requests)
2. **Configure Resilience4j TimeLimiter:**
   ```java
   TimeLimiterConfig config = TimeLimiterConfig.custom()
       .timeoutDuration(Duration.ofSeconds(30))
       .cancelRunningFuture(true)
       .build();
   ```
3. **Add timeout monitoring:**
   - Emit Micrometer metrics for timeout occurrences per dependency
   - Alert if timeout rate exceeds 1% (indicates dependency degradation)
4. **Document timeout values** in API design (Section 5) and deployment configuration

**Reference:** Industry Standard Checklist → Distributed Systems Patterns → "Timeout configurations for all external calls"

---

### S2. Retry Strategy Lacks Exponential Backoff and Jitter

**Location:** Section 2 (Resilience4j), Section 3 (Device Ingestion Service)

**Issue:**
Resilience4j is listed but retry configuration is not specified. Without exponential backoff and jitter:
- Fixed-interval retries create synchronized "thundering herd" patterns when multiple instances retry simultaneously
- Retries during dependency outages overwhelm the recovering service
- No maximum retry limits could lead to infinite retry loops

For example, if PostgreSQL experiences a 10-second outage, all Backend API instances retrying with 1-second fixed intervals create a 10x traffic spike when the database recovers.

**Impact:**
- **Thundering Herd:** Synchronized retries amplify load on recovering dependencies
- **Extended Outages:** Retry storms prevent dependency recovery
- **Reduced Effective Capacity:** Retry load reduces capacity available for new requests

**Countermeasures:**
1. **Configure exponential backoff with jitter:**
   ```java
   RetryConfig retryConfig = RetryConfig.custom()
       .maxAttempts(3)
       .waitDuration(Duration.ofMillis(100))
       .intervalFunction(IntervalFunction.ofExponentialRandomBackoff(
           Duration.ofMillis(100),
           2.0,  // multiplier
           Duration.ofSeconds(5)  // max wait
       ))
       .retryExceptions(TransientException.class)
       .build();
   ```
2. **Define retry policies per dependency:**
   - PostgreSQL transient errors (connection failures): 3 retries with exponential backoff
   - Redis timeouts: 1 retry (cache misses are acceptable)
   - Kafka publish failures: 5 retries (message delivery is critical)
   - S3 5xx errors: 3 retries with exponential backoff
3. **Implement circuit breaker + retry coordination:**
   - Circuit breaker OPEN state should disable retries (fail fast)
   - Retries should only trigger in CLOSED or HALF_OPEN states
4. **Add retry metrics:**
   - Track retry attempts per dependency
   - Alert if retry rate exceeds baseline (indicates dependency instability)

**Reference:** Industry Standard Checklist → Distributed Systems Patterns → "Retry with exponential backoff and jitter"

---

### S3. No Health Check Design at Service Level

**Location:** Section 8 (障害対応とモニタリング), Section 3 (主要コンポーネント)

**Issue:**
While Section 8 lists infrastructure metrics (CPU, memory), there is no mention of application-level health checks:
- Backend API readiness (can serve requests?)
- Backend API liveness (is the process alive but deadlocked?)
- Dependency health (PostgreSQL connection pool healthy?)
- Stream Processor lag (Kafka consumer keeping up?)

Kubernetes requires readiness/liveness probes to manage pod lifecycle. Without proper health checks, Kubernetes may route traffic to unhealthy pods or fail to restart deadlocked processes.

**Impact:**
- **Traffic to Unhealthy Pods:** Requests fail despite healthy pods being available
- **Zombie Processes:** Deadlocked pods consume resources without processing requests
- **Slow Failure Detection:** Kubernetes takes default timeout (30s) to detect unhealthy pods

**Countermeasures:**
1. **Implement health check endpoints:**
   - **Liveness probe:** `GET /actuator/health/liveness` (returns 200 if JVM is alive)
     - Kubernetes config: `initialDelaySeconds: 30, periodSeconds: 10, timeoutSeconds: 5, failureThreshold: 3`
   - **Readiness probe:** `GET /actuator/health/readiness` (returns 200 if dependencies are healthy)
     - Check: PostgreSQL connection pool has available connections
     - Check: Redis is reachable (with 1s timeout)
     - Check: Kafka producer can connect
     - Kubernetes config: `initialDelaySeconds: 10, periodSeconds: 5, timeoutSeconds: 3, failureThreshold: 2`
2. **Add deep health checks** (optional, enabled via query parameter):
   - Execute lightweight DB query (SELECT 1)
   - Check Kafka consumer lag is below threshold
   - Verify S3 bucket accessibility
3. **Implement Kafka Streams health check:**
   - Expose `StreamsBuilderFactoryBean.getKafkaStreams().state()` via endpoint
   - Return unhealthy if state is ERROR or PENDING_SHUTDOWN
4. **Document health check behavior** in Section 6 deployment configuration

**Reference:** Industry Standard Checklist → SRE Best Practices → "Health checks at multiple levels (process, service, infrastructure)"

---

### S4. Undefined Capacity Planning for Auto-Scaling

**Location:** Section 7 (可用性・スケーラビリティ), Section 3 (アーキテクチャ設計)

**Issue:**
Section 7 specifies "CPU使用率70%で自動スケール" but lacks capacity planning details:
- No specification of min/max pod counts per service
- No analysis of per-pod capacity (requests per second per pod)
- No consideration of downstream dependency capacity (PostgreSQL max_connections, Kafka topic partitions)
- No cooldown periods to prevent scaling thrashing

If auto-scaling is too aggressive, rapid scale-up could exhaust PostgreSQL connection pool (default max_connections=100). If too conservative, throughput target (100,000 msg/sec) may not be met.

**Impact:**
- **Connection Pool Exhaustion:** Scaled-up pods exceed database connection limits
- **Scaling Thrashing:** Pods scale up/down rapidly, causing request failures
- **Insufficient Capacity:** Auto-scaling cannot keep up with traffic spikes

**Countermeasures:**
1. **Define per-service capacity model:**
   - **Backend API:** 200 req/sec per pod (estimated from benchmark)
     - Min pods: 3, Max pods: 20
     - PostgreSQL connection pool: 20 connections per pod → 400 max connections (increase RDS max_connections to 500)
   - **Stream Processor:** 5,000 msg/sec per pod (Kafka Streams throughput)
     - Min pods: 5, Max pods: 30 (constrained by Kafka topic partitions)
     - Ensure Kafka topic has 30 partitions for parallel processing
2. **Configure HPA (Horizontal Pod Autoscaler) policies:**
   ```yaml
   apiVersion: autoscaling/v2
   kind: HorizontalPodAutoscaler
   spec:
     scaleTargetRef:
       name: backend-api
     minReplicas: 3
     maxReplicas: 20
     metrics:
     - type: Resource
       resource:
         name: cpu
         target:
           type: Utilization
           averageUtilization: 70
     behavior:
       scaleUp:
         stabilizationWindowSeconds: 60
         policies:
         - type: Percent
           value: 50
           periodSeconds: 60
       scaleDown:
         stabilizationWindowSeconds: 300
         policies:
         - type: Pods
           value: 1
           periodSeconds: 60
   ```
3. **Add capacity monitoring dashboard:**
   - Current pod count vs. max capacity
   - Database connection pool utilization
   - Kafka consumer lag (leading indicator of processing bottleneck)
4. **Document load testing results** and capacity assumptions in Section 7

**Reference:** Industry Standard Checklist → SRE Best Practices → "Capacity planning and load shedding strategies"

---

### S5. Replication Lag Monitoring for TimescaleDB Missing

**Location:** Section 4 (時系列データ), Section 8 (監視項目)

**Issue:**
TimescaleDB is used for time-series data storage but the design does not mention replication configuration or lag monitoring. For a system processing 100,000 msg/sec, write-heavy workloads can cause replication lag between primary and replicas. If Backend API queries read replicas with significant lag:
- Dashboard displays stale sensor data (e.g., 5-minute delay)
- Alerting based on stale data triggers false positives/negatives
- Users cannot trust real-time monitoring claims

**Impact:**
- **Stale Data Presentation:** Dashboards show outdated device status
- **Incorrect Alerting:** Anomaly detection based on lagged data misses real-time issues
- **User Trust Erosion:** Real-time system appears unreliable

**Countermeasures:**
1. **Deploy TimescaleDB with streaming replication:**
   - 1 primary + 2 read replicas (async replication)
   - Configure `max_wal_senders=10` and `wal_keep_size=1GB` on primary
2. **Monitor replication lag:**
   - Query `pg_stat_replication.replay_lag` on primary
   - Alert if lag exceeds 10 seconds (indicates replication bottleneck)
   - Dashboard: Display current replication lag per replica
3. **Implement lag-aware query routing:**
   - For real-time queries (dashboard auto-refresh), query primary
   - For historical analysis queries, allow reading from replicas
   - Add application-level check: if replica lag > 30s, fallback to primary
4. **Add write capacity monitoring:**
   - Track TimescaleDB write throughput (rows/sec)
   - Track `hypertable` compression job lag (TimescaleDB feature)
   - Alert if write queue depth exceeds threshold

**Reference:** Industry Standard Checklist → Data Reliability → "Replication lag monitoring and alerting"

---

## Moderate Issues

### M1. No Rate Limiting for API Endpoints

**Location:** Section 5 (API設計), Section 7 (パフォーマンス目標)

**Issue:**
Section 5 defines public-facing API endpoints (GET /api/v1/devices, POST /api/v1/firmware/updates) but does not specify rate limiting policies. Without rate limits:
- Malicious or misconfigured clients can send excessive requests, degrading service for legitimate users
- Automated scripts without retry backoff can create request storms
- No protection against accidental DDoS from internal services

**Countermeasures:**
1. **Implement tiered rate limiting:**
   - Anonymous requests: 100 req/hour per IP
   - Authenticated users: 1,000 req/hour per user
   - Admin users: 10,000 req/hour per user
2. **Use token bucket algorithm** via API Gateway or Spring Cloud Gateway:
   ```yaml
   spring:
     cloud:
       gateway:
         routes:
         - id: devices-api
           uri: http://backend-api
           filters:
           - name: RequestRateLimiter
             args:
               redis-rate-limiter.replenishRate: 100
               redis-rate-limiter.burstCapacity: 200
               redis-rate-limiter.requestedTokens: 1
   ```
3. **Return 429 Too Many Requests** with `Retry-After` header
4. **Monitor rate limit hits** to detect abuse patterns

**Reference:** Industry Standard Checklist → Infrastructure Resilience → "Rate limiting and backpressure mechanisms"

---

### M2. Firmware Update Rollback Criteria Not Defined

**Location:** Section 3 (Firmware Update Service), Section 4 (device_update_status)

**Issue:**
Section 3 mentions "ロールバック機能" but does not define automated rollback criteria:
- When should updates automatically rollback? (failure rate threshold? timeout?)
- How are partial rollouts handled? (some devices updated, others failed)
- What constitutes a "successful" update? (device reports success? functional validation?)

**Countermeasures:**
1. **Define rollback criteria:**
   - Automatic rollback if failure rate > 10% within first 100 devices
   - Timeout-based rollback: if >20% of devices do not complete within 30 minutes
   - Manual rollback trigger via admin UI
2. **Implement canary deployment:**
   - Phase 1: Update 1% of devices, wait 1 hour, validate metrics
   - Phase 2: Update 10%, wait 1 hour
   - Phase 3: Update 100% if no issues detected
3. **Add rollback procedure:**
   - Mark `device_update_status.status = 'rollback_initiated'`
   - Send downgrade command to affected devices with previous firmware version
   - Track rollback completion in `device_update_status`
4. **Document in Section 6**

**Reference:** Industry Standard Checklist → Operational Safety → "Rollback procedures with rollback criteria"

---

### M3. No Graceful Degradation for Non-Critical Dependencies

**Location:** Section 3 (Backend API → Redis Cache)

**Issue:**
Redis is used for caching but the design does not specify fallback behavior when Redis is unavailable. If cache failures cause API errors, non-critical cache becomes a critical dependency, violating graceful degradation principles.

**Countermeasures:**
1. **Implement cache-aside pattern with fallback:**
   ```java
   try {
       data = redisCache.get(key);
       if (data == null) {
           data = database.query();
           redisCache.set(key, data);
       }
   } catch (RedisException e) {
       logger.warn("Cache unavailable, querying database directly", e);
       data = database.query();
   }
   ```
2. **Define degraded mode behavior:**
   - Redis circuit breaker OPEN → bypass cache, serve from database with increased latency
   - Emit metric `cache_bypass_count` for monitoring
3. **Add cache hit rate monitoring:**
   - Alert if hit rate drops below 70% (indicates cache issues or traffic pattern change)

**Reference:** Industry Standard Checklist → SRE Best Practices → "Graceful degradation and fallback mechanisms"

---

### M4. No Distributed Tracing for Multi-Service Requests

**Location:** Section 3 (データフロー), Section 6 (ロギング)

**Issue:**
Data flows through multiple services (IoT Device → AWS IoT Core → Kafka → Streams Processor → TimescaleDB → API → Frontend) but no distributed tracing system is mentioned. Without trace correlation:
- Debugging latency issues requires manual log correlation across services
- No visibility into end-to-end request flow
- Difficult to identify which service contributes to P99 latency

**Countermeasures:**
1. **Integrate distributed tracing** (OpenTelemetry + Jaeger/Tempo):
   - Instrument all HTTP requests with trace IDs
   - Propagate trace context through Kafka messages (message headers)
   - Track spans: Device → IoT Core → Kafka → Streams → DB → API
2. **Add trace ID to structured logs:**
   ```json
   {
     "timestamp": "2026-02-11T10:00:00Z",
     "level": "INFO",
     "message": "Device data processed",
     "trace_id": "4bf92f3577b34da6a3ce929d0e0e4736",
     "span_id": "00f067aa0ba902b7",
     "device_id": "abc-123"
   }
   ```
3. **Create trace-based dashboards:**
   - P99 latency breakdown by service
   - Error rate per service in trace path
   - Dependency call duration heatmaps

**Reference:** Industry Standard Checklist → Distributed Systems Patterns → "Distributed tracing for debugging production issues"

---

## Positive Aspects

1. **Clear fault isolation boundaries:** Separation of Device Ingestion, Stream Processing, Device Management, and Firmware Update services limits failure blast radius
2. **Resilience4j adoption:** Demonstrates awareness of circuit breaker and retry patterns (though configuration details are missing)
3. **Structured logging with PII masking:** Reduces compliance risk and improves log utility (Section 6)
4. **Defined performance targets:** P95/P99 latency targets provide clear SLO baseline (Section 7)

---

## Summary of Recommendations by Priority

| Priority | Issue ID | Recommendation | Effort |
|----------|----------|----------------|--------|
| Critical | C1 | Implement idempotency for sensor data ingestion | High |
| Critical | C2 | Define circuit breaker configurations for all dependencies | Medium |
| Critical | C3 | Specify transaction boundaries for firmware updates | Medium |
| Critical | C4 | Define Kafka offset commit strategy and poison pill handling | Medium |
| Critical | C5 | Deploy PostgreSQL in Multi-AZ with read replicas | Low (infra config) |
| Critical | C6 | Implement SLO-based alerting thresholds | Medium |
| Critical | C7 | Add database migration backward compatibility procedures | Medium |
| Significant | S1 | Configure timeouts for all external dependencies | Low |
| Significant | S2 | Implement exponential backoff with jitter for retries | Low |
| Significant | S3 | Add readiness/liveness health check endpoints | Low |
| Significant | S4 | Define capacity model and HPA policies | Medium |
| Significant | S5 | Monitor TimescaleDB replication lag | Low |
| Moderate | M1 | Add API rate limiting | Low |
| Moderate | M2 | Define firmware update rollback criteria | Medium |
| Moderate | M3 | Implement cache fallback for Redis failures | Low |
| Moderate | M4 | Integrate distributed tracing | Medium |

**Recommended Immediate Actions:**
1. Address C1-C4 before production deployment (data integrity and cascading failure risks)
2. Implement C5 (SPOF mitigation) as infrastructure prerequisite
3. Define C6 (SLO alerting) before go-live to enable effective incident response
4. Add S3 (health checks) to support Kubernetes orchestration

This review covers all industry standard reliability patterns from the evaluation checklist and maps findings to specific design document sections. The critical issues represent risks that could prevent achieving the 99.9% availability target or cause data integrity failures under production load.
