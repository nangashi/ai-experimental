---
scope: LLM出力の評価・採点・信頼性検証の仕組み設計
boundary: |
  LLM出力の評価・採点・信頼性検証の仕組み設計を対象とする。
  ランタイムガードレールフレームワークは対象外。
  AI生成コードの品質指標は対象外。
research_query: LLMを評価者として使用する際の信頼性・正確性に関する研究データと補正手法、ハルシネーション検出・軽減の定量データ
---

# LLM出力の評価・採点・信頼性検証の仕組み設計

LLM出力の評価・採点・信頼性検証の仕組み設計に関する研究・実践知見。

## LLM-as-Judgeの12種バイアス

LLMを評価者として使用する際の体系的バイアスが12種類カタログ化されている。

### 特に影響の大きいバイアス

**位置バイアス**: 提示順序の入れ替えで精度が10%以上変動する。

**冗長性/長さバイアス**: 内容の質に関係なく、冗長でフォーマルな出力を高評価する。

**自己選好バイアス**: パープレキシティが低い（モデルにとって「馴染みのある」）テキストを高評価する。本質は著者ではなくパープレキシティ。

**スコアリングバイアス（3サブタイプ）**: 以下の3要素がそれぞれ**独立に**判定を変動させる。
1. ルブリックの順序（項目の提示順）
2. スコアIDのラベル（数値の割り当て方）
3. 参照回答の有無

GPT-4oでも摂動により人間との相関が最大0.2変動する。

## CoT忠実性の定量データ

CoTの推論ステップがモデルの実際の推論過程を反映しているかを定量測定した結果。

- **忠実性はバイアスタイプで劇的に異なる**: Sycophancy型ヒント: 60%忠実。報酬ハッキング型ヒント: 0%忠実
- **不忠実なCoTは体系的に長い**: Claude 3.7 Sonnet — 不忠実: 2,064トークン vs 忠実: 1,439トークン。DeepSeek R1 — 不忠実: 6,003 vs 忠実: 4,737
- **「CoTを必要としないタスクでCoTモニタリングの安全性を主張することは難しい」**

**設計指針**: 長い推論 = 良い推論ではなく、逆の可能性がある。出力は推論トレースではなく最終結果で検証すべき。

## キャリブレーション過信

- **84.3%のシナリオで過信**（9 LLM、351シナリオ中296で過信）
- 最も精度の高いモデルでも正解時と不正解時の**信頼度にほとんど差がない**
- **推論強化はキャリブレーションを悪化させる**: 「より深く考えさせる」と信頼度の推定がより不正確になるという反直感的な結果

**ディストラクター効果**: もっともらしい誤答選択肢を明示的に含めると精度が最大460%改善、キャリブレーション誤差が90%減少する。

**設計指針**: LLMの自己評価の信頼度スコアを鵜呑みにしない。評価タスクではもっともらしい誤答を含めることで精度が大幅に向上する。

## 自己レビューの盲点

コードを生成したのと同じモデルがレビューする場合、自分のミスを検出できない傾向がある。これは自己選好バイアスの実践的帰結。

- 生成と同じモデルでのレビューはコードの「もっともらしさ」に対する閾値が低い
- 自分が生成したパターンを「正しい」と認識する傾向
- 人間が読めば気づく不整合を見逃す

**設計指針**: レビュー用エージェントと生成用エージェントを分離する。セルフレビュー時は「このコードを初めて見る人の視点で」と明示的にフレーミングする。

## 補正手法

### 提示順序のランダム化

ペアワイズ比較では提示順序をランダム化すべき。固定順序は位置バイアスの影響を系統的に受ける。

### 生成と評価のモデル分離

生成と評価で同一モデルを使わない。自己選好バイアスを構造的に排除する。

### 匿名化

回答からソース帰属を除去するとidentity-driven biasを防止できる。並列レビューでクロス評価する前に回答を匿名化する。

### 批判的評価フレーミング

「評価する」ではなく「批判的に評価し、代替案を提案する」とフレーミングすることでsycophancy傾向に対抗する。

### ルブリック・スコア設計の注意

ルブリックの順序とスコアIDラベルが独立にバイアスを発生させることを前提に評価設計すべき。出力長で正規化してバリアント比較する。

## ハルシネーション率の定量データ

### モデル別ベースライン

- ベストモデルでも最低**0.7%**のハルシネーション率（Vectara 2025年12月、7,700+記事評価）
- ワーストモデルでは**25%超**
- タスク領域で劇的に変動する

### タスク別ハルシネーション率

| タスク領域 | ハルシネーション率 | 備考 |
|-----------|-----------------|------|
| 医療/臨床 | 53.3〜82.7% | GPT-4oが53.3%で最良、DeepSeekは80-82.7% |
| 知識グラウンディング | 約45% | GPT-4o、拒否しない場合 |
| 臨床テキスト要約（最適化済み） | 1.47% | 専用フレームワーク使用時 |

ハルシネーション率はタスク領域とモデルの組み合わせで大きく変わるため、ユースケース固有の測定が必須。

### 軽減効果の定量データ

- キュレーション済み訓練データ: 生データ比**40%削減**（MIT 2025）
- 組み込み推論能力: 最大**65%削減**（Google 2025）
- ハイブリッドRAG+統計検証: **97%検出率**、200ms未満のレイテンシ

## ハルシネーション検出手法

### Faithfulness評価

生成された回答が参照コンテキストに基づいているかを測定する。RAGの文脈では「コンテキスト↔レスポンス」の忠実性として定義される。

- 回答の各文が引用パッセージによって裏付けられているか
- 引用が主張された事実を実際に含んでいるか
- 裏付けのない主張にペナルティを課す

### 引用マッピング検証

生成された引用が実際のソースと正しく対応しているかを検証する。span-levelの検証で、引用の正確性を文単位で測定可能。

### ベンチマーク

- **HHEM（Hughes Hallucination Evaluation Model）**: Vectaraの商用・OSSモデル。グラウンド要約と内在的ハルシネーションに焦点
- **HalluLens**: 外在的・内在的ハルシネーションの分類体系を含む評価タスク

## ハルシネーション軽減手法

### グラウンディング（RAG）

外部知識ソースを検索し、回答生成時の根拠として使用する。「オープンブック回答」方式。

- 単独でもハルシネーション率を大幅に低下させる
- ただしRAGはハルシネーションを**排除するのではなく低減する**
- ハルシネーション源: Knowledge FFNがパラメトリック知識を過度に重視し、Copying Headsが外部知識の統合に失敗する

### Self-Consistency検証

複数の推論パスを生成し、最も一致する回答を選択する手法。評価・検証の文脈では、単一の推論結果を鵜呑みにせず複数パスの合意で信頼性を担保する。

- GSM8K: **+17.9%**、SVAMP: **+11.0%**、AQuA: **+12.2%**
- 算術・常識推論で特に有効（推論時コンピュート拡張としての側面は「LLMへの指示の書き方」で詳述）

### ディストラクター効果との接続

「キャリブレーション過信」セクションで述べた通り、もっともらしい誤答選択肢を明示的に含めると精度が最大460%改善する。この手法はハルシネーション軽減にも直接応用でき、評価タスクで「もっともらしい誤りの例」を提示することで、モデルがより慎重に回答を検証するようになる。
