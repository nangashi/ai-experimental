---
scope: AI出力の品質管理と安全性
boundary: |
  AI出力の品質管理と安全性を対象とする。
  LLM出力の評価・採点の仕組み設計は対象外。
research_query: AI出力の品質特性・リスク・改善手法、ガードレール・安全性に関する定量データと実践知見
---

# AI出力の品質管理と安全性

AI出力の品質管理と安全性に関する研究・実践知見。

## 定量的品質データ

複数の大規模調査から、AI生成コードの品質問題が定量的に明らかになっている。

- AI生成コードは人間コードより**1.7倍多くの問題**を含む
- **保守性・品質エラーが1.64倍**高い
- **コード重複が4倍**に増加（コピー＆ペースト的な生成傾向）
- セキュリティ脆弱性を含む確率が**2.74倍**
- AI生成PRの**67.3%が却下**される（手動コードは15.6%）
- Google DORA 2025: AI採用90%増加に対し、バグ率9%増、コードレビュー時間91%増、PRサイズ154%増

ただし厳格なプロセスを導入したチームは、コード出力5倍増にもかかわらず**本番ホットフィックスを約40%削減**している。プロセスの有無が品質を決定する。

## 80%問題: 3つのアンチパターン

Google ChromeチームのAddy Osmaniによる分析。「AIは80%まで速く到達するが、残り20%で品質を破壊する」問題を3つのアンチパターンとして特定。

### 抽象化肥大 (Abstraction Bloat)

- 100行で十分なところに1,000行を生成する
- 関数で済むところに精巧なクラス階層を構築する
- 開発者が積極的にこの傾向に抵抗する必要がある

### デッドコード蓄積 (Dead Code Accumulation)

- エージェントは自分が生成した不要コードを片付けない
- 古い実装を残したまま新しい実装を追加する
- コメントを副作用的に削除する
- タスクの近くにあるという理由だけで、理解していないコードを変更する

### 追従的実行 (Sycophantic Execution)

- 不完全・矛盾する要件に対して確認を求めず熱心に実行する
- 間違った仮定で突き進む
- 批判的な質問を投げかけない

## 仮定伝播 (Assumption Propagation)

モデルが序盤で誤った仮定を立て、その仮定の上に機能全体を構築するパターン。問題は複数のPRにまたがって初めて発覚する。

**メカニズム**:
1. 要件の曖昧な部分についてモデルが確認せず仮定を置く
2. 仮定に基づいてコードを生成、テストも通過する（仮定に整合的なテストを書くため）
3. 後続のタスクでも同じ仮定が前提として継承される
4. 実際のユースケースとの乖離が蓄積し、大規模な修正が必要になる

**対策**: 曖昧な要件に対して仮定を置く前にユーザーに確認する。仮定を置いた場合は明示的に文書化し後続タスクで検証可能にする。特に設計判断・API設計・データモデルなど後続タスクへの影響が大きい領域で注意する。

## PR失敗パターン

33,000+ PR分析から判明したAI生成PRの特徴。

- ドキュメント/CI/ビルドタスクのマージ率が最高。パフォーマンス・バグ修正タスクは最も失敗
- 失敗PRの特徴: 変更が大きい、ファイル数が多い、CI失敗
- 22%がコードレベル問題、17%がCI/テスト失敗

**必要な4つの改善**:
1. 既存/進行中の作業を特定する
2. プロジェクト貢献規範に従う
3. ローカライズされた変更に分解する
4. 提出前にCIで検証する

メンテナーのフィードバック: 「PRは小さく、焦点を絞り、単一の一貫した変更に限定すべき」

## セキュアコード生成

- AI生成コードの45%がセキュリティテストに不合格
- 86%がXSS防御に失敗(CWE-80)、88%がログインジェクションに脆弱(CWE-117)
- **Claude Opus 4.5 + Thinking**: セキュリティプロンプティングなしで56%のセキュアコード生成率。特定のCWE番号を指定して回避を指示すると**69%に向上(+13pp)**

**設計指針**: セキュリティレビューにはgenericな「セキュリティを確認せよ」ではなく、具体的なCWE番号を含めると検出率が向上する。

## コードチャーンの品質シグナル

コードチャーン（書いた直後に修正・書き直しされるコードの割合）がAI時代の品質指標として注目されている。

- 従来のメトリクス（LoC、PR数）はAI時代に膨張して無意味化する
- チャーンは「素早く書かれたコード」と「安定したロジック」を区別できる
- AI出力の増加 + チャーン上昇 = 実質的な生産性は低下

**設計指針**: 繰り返し修正が必要なコードパターンを検出したら生成アプローチを変更する判断基準にする。

## Eval駆動開発

スキル/エージェント構築**前に**評価基準を定義するアプローチ。TDDのエージェント版。

### 評価の4カテゴリ

1. **成果目標**: タスクが完了したか
2. **プロセス目標**: 意図したツール/ステップを使ったか
3. **スタイル目標**: 出力が規約に従っているか
4. **効率目標**: 不要なコマンドなしで到達したか

### 実践手法

- 3-5メトリクスを使用する
- コンポーネントレベル（ツール正確性、パラメータ精度）とエンドツーエンド（タスク完了）を混合する
- 推論層とアクション層を**別々に**評価する
- 自動eval（CI/CD前）+ 本番モニタリング（ドリフト検出）+ A/Bテスト + 手動トランスクリプトレビュー

## セキュリティ: プロンプトインジェクション防御

OWASP 2025 LLMアプリケーション Top 10 第1位の脆弱性。

### 多層防御戦略

1. **入力ガードレール**: 悪意ある要素の事前検出
2. **コンテンツフィルタリング**: 埋め込みベースの異常検出
3. **階層的システムプロンプト**: 行動契約としての構造化プロンプト
4. **出力検証**: 本来含まれるべきでない情報のスキャン
5. **アクションガード**: 高リスクアクションの動的権限チェック

多層防御フレームワークで攻撃成功率73.2%から8.7%に低減（タスク性能の94.3%を維持）。

**注意点**: 同じLLMを生成とセキュリティ評価の両方に使うと複合脆弱性になる。単純な手法（感情操作、タイポ、難読化）は依然として最新モデルを回避可能。

## 出力ガードレールフレームワーク

LLM出力の品質と安全性をランタイムで制御するフレームワーク。

### NeMo Guardrails（NVIDIA）

プログラマブルなガードレールのためのオープンソースツールキット。

- **5種のレール**: Input、Dialog、Output、Retrieval、Execution
- 推論対応コンテンツ安全性モデル、多言語コンテンツ安全性（自動言語検出）
- BotThinkingイベントによるLLM推論トレースのガードレール
- LangChain、LangGraph、LlamaIndex統合
- エージェント、マルチターン会話、クリティカルワークフロー向けの細粒度制御

### Guardrails AI

入出力バリデーションに特化したオープンソースフレームワーク。

- PII検出、毒性フィルタリング、ハルシネーション防止、正規表現マッチングから複雑なチェックまで
- Guardrails Index: 6カテゴリ24ガードレールを比較する業界初のベンチマーク（2025年2月）
- NeMoのステートマシンアプローチと補完関係にある

### 多層防御パターン

ガードレールは単一の防御層ではなく、複数層の組み合わせで設計する。

1. **入力バリデーション・サニタイズ**: 有害・操作的プロンプトのフィルタリング
2. **ランタイムモニタリング・検出**: リアルタイムの行動監視
3. **出力フィルタリング・検証**: 生成結果の品質・安全性チェック
4. **透明性・信頼度スコアリング**: ユーザーへの信頼度情報提供

ハイブリッドRAG+統計検証による多層防御で**97%検出率**、200ms未満レイテンシを達成。

**設計指針**: 入力側のプロンプトインジェクション防御だけでなく、出力側のバリデーションも必須。単一の防御層に依存しない。

## 構造化出力の安全性

### スキーマドリフト防止

スキーマドリフト（LLMが期待されるJSON構造から逸脱する現象）は自動化パイプライン破損の最大原因。

- `strict: true` フラグで余分なフィールドとフォーマットドリフトを防止
- Structured Outputs（制約付きデコーディング）でスキーマ準拠を100%保証
- 重要なプロンプトには自己チェックブロックを付加して出力フォーマット準拠を検証させる

### 出力インジェクション防止

LLM生成の出力を下流システムにサニタイズなしで渡すことで発生する脆弱性（OWASP LLMリスク）。

- **リスク**: XSS、SQLインジェクション、リモートコード実行
- **根本原因**: モデルを信頼済みソースとして扱い、出力を検証しない
- **対策**: LLM出力を下流システムに渡す前に必ずサニタイズ。構造化出力のフォーマット検証。サンドボックスでの実行

**設計指針**: LLM出力は常に「未信頼入力」として扱い、下流システムへの受け渡し前にバリデーションを行う。

## コンテンツモデレーション

### Llama Guard（Meta AI）

オープンソースのコンテンツ安全性モデル。

- Llama Guard 3: 8言語対応（英語、フランス語、ドイツ語、ヒンディー語、イタリア語、ポルトガル語、スペイン語、タイ語）
- Llama Guard 4（12B）: 複雑なプロンプト/レスポンスに対する推論性能向上

### Amazon Bedrock Guardrails

6つの設定可能なセーフガードポリシー。

1. コンテンツモデレーション（コンテンツ・ワードフィルタ）
2. プロンプト攻撃検出
3. トピック分類（拒否トピック）
4. PII編集（機密情報フィルタ）
5. ハルシネーション検出
6. 画像コンテンツフィルタ（有害マルチモーダルコンテンツの最大**88%**をブロック）

**設計指針**: ルールベースフィルタリング（Bedrock）とモデルベース検出（Llama Guard）の組み合わせが効果的。マルチモーダル対応が必要な場合は画像フィルタも検討する。
