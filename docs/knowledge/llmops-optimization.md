---
scope: LLMアプリケーションの運用・コスト・レイテンシ最適化
boundary: |
  LLMアプリケーションの運用・コスト・レイテンシ最適化を対象とする。
  LLMに渡すコンテキストの設計と管理は対象外。
research_query: LLMアプリケーションの運用基盤、コスト削減、レイテンシ改善、モデル選択、デプロイメント戦略に関する定量データと実践知見
---

# LLMアプリケーションの運用・コスト・レイテンシ最適化

LLMアプリケーションの運用・コスト・レイテンシ最適化に関する研究・実践知見。

## オブザーバビリティ

### 計測すべきメトリクス

**テクニカルメトリクス**:
- First Token Latency（TTFT）: 最初のトークン生成までの時間
- Per Token Latency / Intertoken Latency（ITL）: 後続トークン間の時間
- Tokens Per Second（TPS）
- Requests Per Second（RPS）

**LLM固有メトリクス**:
- トークン使用量とコスト帰属
- レスポンス品質スコア
- 安全性ガイドライン遵守率
- ハルシネーション率

**ビジネスメトリクス**:
- タスク成功率
- ユーザー満足度
- リクエスト/タスクあたりコスト

### OpenTelemetryトレーシング

GenAIセマンティック規約が標準化進行中（Development status）。イベント、メトリクス、モデルスパン、エージェントスパンをカバー。

- Datadog: OpenTelemetry GenAI Semantic Conventions（v1.37+）をネイティブサポート
- Azure AI Inference、OpenAI、AWS Bedrock向けのテクノロジー固有規約あり
- エージェントシステム（タスク、アクション、エージェント、チーム、アーティファクト、メモリ）への拡張提案が進行中

**設計指針**: OpenTelemetry互換のトレーシングを採用すると、ベンダー柔軟性と既存オブザーバビリティスタックとの統合を確保できる。

### 主要ツール

**OSS**: Langfuse（MIT、マルチターン対応トレーシング+プロンプトバージョニング）、OpenLLMetry（OpenTelemetry拡張）、LangWatch（OpenTelemetryネイティブ）

**マネージド**: Datadog（統合オブザーバビリティ）、Arize（ML/LLMモデル評価）

## コスト最適化

戦略的最適化により性能を犠牲にせずLLMコストを**最大80%削減**可能。学術研究では推論コストを**最大98%削減**しつつ精度を向上させた事例もある。

### プロンプトキャッシュ

同一プレフィックスを持つリクエストのキャッシュにより、コストとレイテンシを大幅に削減する。

**プロバイダ別効果**:

| プロバイダ | コスト削減 | レイテンシ削減 |
|-----------|----------|--------------|
| Anthropic | 最大90% | 最大85% |
| OpenAI | 50%（デフォルトで自動有効） | — |

- Anthropic: キャッシュ書き込みは基本入力料金の**1.25倍**、キャッシュ読み取りは**0.1倍**（90%割引）
- 実例: エンタープライズ文書QA（1,000クエリ/日、10文書×20Kトークン）で年間**$20,000節約**
- 実例: 月額APIコスト**$720→$72**（90%削減）

**適用条件**: LLMクエリの**31%**がセマンティックに類似 — キャッシュなしでは大きな非効率。

### モデルルーティング

タスク複雑度に応じて適切なモデルティアに振り分ける。

- カスケードルーティング（ルーティング戦略の組み合わせ）が単独戦略を**最大14%**上回る
- 信頼性の高いジャッジ（>80%信頼性）で**5倍以上のコスト削減**、性能劣化ゼロまたは無視可能
- ジャッジ信頼性が約80%を下回ると性能が急速に低下

**設計指針**: 単純なタスクにプレミアムモデルを使わない。ルーティングジャッジの信頼性が80%以上であることを確認する。

### KVキャッシュ最適化

KVキャッシュ対応ルーティングで**87%キャッシュヒット率**、ウォームキャッシュヒット時**88%高速化**（TTFT）。関連コンテキストをGPUメモリに保持するポッドにリクエストを誘導する。

## レイテンシ最適化

### Speculative Decoding

小型の高速ドラフトモデルが複数トークンを先行提案し、大型ターゲットモデルが並列検証する。

- **2-3倍のレイテンシ改善**、数学的に同一の出力品質を維持
- 最適ドラフトモデルで**最大3倍**高速化
- Decentralized Speculative Decoding（DSD）: 追加で**15-20%のエンドツーエンド高速化**

**注意**: 実環境では性能が脆弱で変動が大きい。ワークロード特性、バッチサイズ、モデル構成、システム条件に依存する。

### バッチング最適化

**Continuous Batching**: LLM推論スループットを**最大23倍改善**、p50レイテンシ削減。

**Sarathi-Serve**: チャンク化プリフィル（〜512トークン分割）+ハイブリッドバッチングで、デコードを中断させない。Falcon-180Bで厳密SLO下**4.3倍のキャパシティ増加**。

**コアトレードオフ**: 大バッチ（高スループット）vs 小バッチ（低レイテンシ）。プリフィル（計算集約的）とデコード（メモリバウンド）の混合バッチは**8-10倍の速度低下**を引き起こすため、フェーズ分離設計が有効。

## モデル選択戦略

### タスク別ティア選択

| ティア | 用途 | 代表モデル |
|-------|------|----------|
| 汎用リーダー | コーディング、推論、長コンテキスト | GPT-5、Claude Opus/Sonnet |
| 速度/コスト重視 | チャットボット、高ボリュームAPI | Gemini Flash、Llama 3.3、DeepSeek V3 |
| 専門特化 | ドメイン固有タスク | 各ドメインで最適なモデルを選択 |

### 市場シェア（2025年中期エンタープライズ）

- Claude: 32%、OpenAI: 25%、Gemini: 20%
- クローズドソース合計: 87%

**設計指針**: ベンダーロックインを避けるマルチモデル戦略が主流化。自動フェイルオーバーを含むマルチプロバイダ構成を検討する。

## デプロイメントパターン

### カナリアデプロイメント

段階的ロールアウト: **5%→10%→25%→50%→100%**。各段階でモニタリングし、品質メトリクス・レイテンシ・エラー率が劣化した場合は自動ロールバック。

### シャドウテスト

新しいLLMを本番リクエストで評価するが、ユーザーには影響しない。ライブトラフィックを候補モデルに複製しレスポンスをログ。ユーザーは既存モデルの出力のみ受け取る。実環境データでのリスクフリーな検証が可能。

### A/Bテスト

異なるプロンプト、モデル、チェーン構成をテスト。特定ユーザーセグメントをカナリア版に誘導し、全トラフィック移行前にライブ性能を測定する。

**設計指針**: プロンプト変更、モデル変更、チェーン構成変更のいずれも、段階的ロールアウトとモニタリングを組み合わせてデプロイする。一括切り替えを避ける。
