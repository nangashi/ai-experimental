---
scope: LLMに渡すコンテキストの設計と管理
boundary: |
  LLMに渡すコンテキストの設計と管理を対象とする。
  指示の書き方（テクニック選択・構造化）は対象外。
  複数エージェント間の通信設計は対象外。
research_query: LLMに渡す情報の設計・管理に関して、出力品質や効率に影響を与える手法や研究データ
---

# LLMに渡すコンテキストの設計と管理

LLMに渡すコンテキストの設計と管理に関する研究・実践知見。

## コンテキストエンジニアリングの基本

2025年半ばに台頭した概念で、プロンプトエンジニアリングの進化形に位置づけられる。

| 観点 | プロンプトエンジニアリング | コンテキストエンジニアリング |
|------|------------------------|------------------------|
| 焦点 | 言葉遣い、構造、テクニック | モデルの作業記憶に何を入れるか |
| 範囲 | 単一インタラクション | 多様なソースからの動的組み立て |
| 構成要素 | プロンプトテキスト | プロンプト + メモリ + RAG + ツール出力 + ガードレール |

**核心的な知見**: プロンプトエンジニアリング改善の85%はコンテキスト改善で達成可能。テクニックを工夫する前にまず適切な情報を提供する。

**Just-in-Time戦略**: 必要時に動的にロードする。全データの事前ロードではなく、基本コンテキストは事前取得+追加探索は必要時に実行するハイブリッドアプローチが有効。

## Context Rot: 情報量と位置による精度劣化

Chroma Researchの18 LLM評価による大規模実証。

- 1,000トークンでは各トークンが999トークンに注意。100,000トークンでは99,999トークンに注意が分散
- **わずか20件の検索文書（〜4,000トークン）で精度が70-75%から55-60%に低下**
- **位置依存性**: 位置1の事実は約75%精度、位置10は約55%精度

「情報がコンテキストに存在するかどうかは十分ではない。**どのように、どこに**提示されるかがより重要」

**設計指針**:
- 最重要な指示・コンテキストをプロンプトの先頭と末尾に配置
- 無関係なコンテキストを積極的に除去する
- コンテキスト追加のたびに既存情報の精度低下を意識する

## マルチターン劣化のメカニズムと対策

200,000+のシミュレーション会話に基づく研究で、シングルターンからマルチターンへの移行で**平均39%の性能低下**が確認された。

### 4つの原因

1. **早期解決試行**: 要件が揃う前に仮定を置いて完全な回答を生成してしまう
2. **回答アンカリング**: 過去の（誤っている可能性のある）回答に過度に依存し、肥大化した回答を生成
3. **中間ターン忘却**: 最初と最後のターンを過度に重視し、中間のターンを軽視
4. **冗長性カスケード**: 回答が次第に冗長化し、仮定がユーザーの発言を圧倒

### 対策

**Concat-and-Retry**: マルチターンで蓄積した情報を統合し、クリーンな単一プロンプトとして新鮮なインスタンスに送信。精度が90%超に回復し、シングルターン性能にほぼ一致する。Claude Codeのauto-compactionはこの形式化に相当する。

**Mediator-Assistantアーキテクチャ**: 専用の「Mediator」エージェントが曖昧なマルチターン入力を明示的な構造化指示に変換してからタスク実行「Assistant」に渡す。

**設計指針**:
- 長いマルチフェーズワークフローでは、蓄積された決定事項を新鮮なプロンプトに統合する「リセットポイント」を設計する
- 「すべての要件が提示されるまで完全な解決策を提案しない」という指示を含める
- 2回の修正失敗後はセッションを新規にして書き直す方が効率的

## コンテキスト圧縮: 観察マスキング vs 要約

コンテキスト管理の2つの主要アプローチが実践で分化している。

### 観察マスキング (Observation Masking)

- ツール出力やファイル内容の不要部分を選択的に除去
- 完全な情報を保持しつつ不要なトークンを消費しない
- Cursor, Warpが採用。より洗練されたアプローチ

### LLM要約 (Summarization)

- 別のLLMが古いコンテキストを要約して圧縮
- 実装は簡単だが、要約時に詳細が失われるリスク
- Claude Codeのauto-compactionはこのアプローチの一形態

### 実践データ

- Riceプラットフォーム: 長期記憶+短期状態管理の統合で**コンテキスト消費60%削減**
- 「メモリはエージェントの機能ではなく**インフラ**」

**設計指針**: ツール出力が大きい場合、全体をコンテキストに入れるのではなく関連部分のみを選択的に保持する。

## MCPのコンテキストコスト

MCPサーバー、ツール定義、システムプロンプトがコンテキストウィンドウの一定割合を常に消費している。

- MCPツール定義だけで数千トークンを消費する場合がある
- ツールが多いほどコンテキストの実効サイズが小さくなる
- 「MCPサーバーがコンテキストコストに見合うか」を評価すべき

**設計指針**: 多数のMCPツールを同時に有効化することのトレードオフを認識し、使用頻度の低いツールはJust-in-Timeでロードする。

## 指示ファイルの最小化原則

Anthropic公式のCLAUDE.mdベストプラクティスに基づく。

- 「各行について問え: これを削除するとClaudeがミスするか？ NOなら削除」
- 含めるべきでないもの: コードを読めば分かること、標準的な言語規約、詳細なAPIドキュメント、「クリーンなコードを書け」等の自明な指示
- 「肥大化したCLAUDE.mdファイルはClaudeに実際の指示を無視させる」

## ACE: grow-and-refine原則

コンテキストを「進化するプレイブック」として扱うフレームワーク。

**3つの専門化された役割**:
- Generator: 推論軌跡を生成
- Reflector: 評価と洞察抽出
- Curator: 有用/有害カウンター付き構造化デルタ更新、決定論的マージ、重複排除、プルーニング

**grow-and-refine原則**: 全面書き換えではなくインクリメンタル更新。「簡潔化バイアス」（ドメイン洞察を要約のために削除）と「コンテキスト崩壊」（反復的書き換えで詳細が劣化）を防止する。

**定量結果**: エージェントタスク+10.6%、金融タスク+8.6%。適応レイテンシ82-92%削減、トークンコスト75-84%削減。

## 3層アーキテクチャ

学習能力を持つコーディングエージェントのためのメモリ設計。

1. **Hooks層（ハードルール）**: 許可/拒否を決定論的に制御。危険なコマンドのブロック、フォーマット強制
2. **Working Memory層（短期記憶）**: 現在のセッション内の状態、中間結果、決定事項
3. **Long-term Knowledge層（長期知識）**: セッション間で永続する学習結果、パターン、教訓

**設計指針**: `memory/`（永続的事実）と`outputs/`（一時的結果）を分離する。永続メモリはコンテキストウィンドウの外に保存し、関連するものだけロードする。
