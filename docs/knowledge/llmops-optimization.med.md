# Instruction Extract: llmops-optimization

source: /home/toyama-ryosuke/ghq/github.com/nangashi/ai-experimental/docs/knowledge/llmops-optimization.md
extracted: 2026-02-16
items: 9

---

## KE-001: OpenTelemetry互換トレーシングの採用

- **use-when**:
- **proposed-use-when**: LLMアプリケーションの観測基盤を設計するとき
- **scope**: LLMアプリケーションのトレーシングシステム選定
- **action**: OpenTelemetry互換のトレーシングを採用する。GenAIセマンティック規約（v1.37+）をサポートするツールを優先する。
- **rationale**: OpenTelemetry GenAI Semantic ConventionsはDatadog、Azure AI Inference、OpenAI、AWS Bedrockで標準化が進行中であり、ベンダー柔軟性と既存オブザーバビリティスタックとの統合を確保できる。

---

## KE-002: プロンプトキャッシュの適用

- **use-when**: スキルやマルチステップのAIワークフローを設計するとき
- **scope**: 同一プレフィックスを持つリクエストが複数存在するLLMアプリケーション
- **action**: プロンプトキャッシュを有効化する。Anthropicでは最大90%コスト削減、最大85%レイテンシ削減が期待できる。
- **rationale**: LLMクエリの31%がセマンティックに類似しており、キャッシュなしでは大きな非効率が発生する。実例でエンタープライズ文書QAで年間$20,000節約、月額APIコスト$720→$72（90%削減）を達成。Anthropicではキャッシュ書き込みは基本入力料金の1.25倍、キャッシュ読み取りは0.1倍（90%割引）。

---

## KE-003: モデルルーティングジャッジの信頼性確保

- **use-when**:
- **proposed-use-when**: タスク複雑度に応じたモデルルーティングを設計するとき
- **scope**: モデルルーティングシステムの設計
- **action**: ルーティングジャッジの信頼性が80%以上であることを確認する。信頼性の高いジャッジ（>80%信頼性）を使用する。
- **rationale**: 信頼性の高いジャッジ（>80%信頼性）で5倍以上のコスト削減、性能劣化ゼロまたは無視可能を達成。ジャッジ信頼性が約80%を下回ると性能が急速に低下する。カスケードルーティング（ルーティング戦略の組み合わせ）が単独戦略を最大14%上回る。

---

## KE-004: KVキャッシュ対応ルーティングの導入

- **use-when**:
- **proposed-use-when**: LLMアプリケーションのレイテンシ改善手法を選択するとき
- **scope**: 関連コンテキストを再利用するLLMアプリケーション
- **action**: KVキャッシュ対応ルーティングを導入し、関連コンテキストをGPUメモリに保持するポッドにリクエストを誘導する。
- **rationale**: 87%キャッシュヒット率を達成し、ウォームキャッシュヒット時にTTFTを88%高速化できる。

---

## KE-005: Speculative Decodingの慎重な適用

- **use-when**:
- **proposed-use-when**: LLMアプリケーションのレイテンシ改善手法を選択するとき
- **scope**: レイテンシ改善のためのSpeculative Decoding導入検討
- **action**: Speculative Decodingを導入する場合、ワークロード特性、バッチサイズ、モデル構成、システム条件を十分に検証する。最適ドラフトモデルを選定する。
- **rationale**: 理論上2-3倍のレイテンシ改善、最適ドラフトモデルで最大3倍高速化、数学的に同一の出力品質を維持できる。Decentralized Speculative Decoding（DSD）で追加15-20%のエンドツーエンド高速化が可能。
- **conditions**: 実環境では性能が脆弱で変動が大きく、ワークロード特性、バッチサイズ、モデル構成、システム条件に依存するため、期待通りの性能が出ない可能性がある。

---

## KE-006: Continuous Batchingとフェーズ分離設計

- **use-when**:
- **proposed-use-when**: LLM推論システムのバッチング戦略を設計するとき
- **scope**: LLM推論スループットとレイテンシの最適化
- **action**: Continuous Batchingを採用する。プリフィル（計算集約的）とデコード（メモリバウンド）を混合しない設計を行う。Sarathi-Serveのようなチャンク化プリフィル（〜512トークン分割）+ハイブリッドバッチングを検討する。
- **rationale**: Continuous BatchingによりLLM推論スループットを最大23倍改善、p50レイテンシ削減を達成。プリフィルとデコードの混合バッチは8-10倍の速度低下を引き起こす。Sarathi-ServeはFalcon-180Bで厳密SLO下4.3倍のキャパシティ増加を実現。

---

## KE-007: タスク別モデルティア選択

- **use-when**: スキルやマルチステップのAIワークフローを設計するとき
- **scope**: タスク複雑度に応じたモデル選択
- **action**: 単純なタスクにプレミアムモデルを使わない。タスク別に適切なティア（汎用リーダー、速度/コスト重視、専門特化）を選択する。
- **rationale**: 戦略的最適化により性能を犠牲にせずLLMコストを最大80%削減可能。学術研究では推論コストを最大98%削減しつつ精度を向上させた事例もある。

---

## KE-008: マルチモデル戦略と自動フェイルオーバー

- **use-when**:
- **proposed-use-when**: LLMアプリケーションのベンダー戦略を設計するとき
- **scope**: LLMプロバイダ選定とアーキテクチャ設計
- **action**: ベンダーロックインを避けるマルチモデル戦略を採用する。自動フェイルオーバーを含むマルチプロバイダ構成を検討する。
- **rationale**: 2025年中期エンタープライズ市場ではClaude 32%、OpenAI 25%、Gemini 20%とシェアが分散しており、クローズドソース合計87%。マルチモデル戦略が主流化している。

---

## KE-009: 段階的ロールアウトとモニタリング

- **use-when**:
- **proposed-use-when**: LLMアプリケーションのプロンプト・モデル・チェーン構成を変更するとき
- **scope**: プロンプト変更、モデル変更、チェーン構成変更のデプロイメント
- **action**: 段階的ロールアウト（5%→10%→25%→50%→100%）とモニタリングを組み合わせてデプロイする。各段階で品質メトリクス・レイテンシ・エラー率を監視し、劣化した場合は自動ロールバックする。一括切り替えを避ける。
- **rationale**: カナリアデプロイメント、シャドウテスト、A/Bテストを組み合わせることで、実環境データでのリスクフリーな検証が可能。段階的ロールアウトにより、問題発生時の影響範囲を最小化できる。
